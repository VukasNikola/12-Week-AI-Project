{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b906350c",
   "metadata": {},
   "source": [
    "Just a recap that when we were doing regression we had a column vector y so if we wanted the output to be of size (n_samples,1) and the weight vector is the same size then the X has to be of size (n_samples,n_features)\n",
    "Exactly‚Äîspot on! Whenever you want one prediction per sample, you‚Äôll aim for a result of shape `(n_samples,)` (or `(n_samples, 1)` if you keep it 2-D). In practice that means:\n",
    "\n",
    "```text\n",
    "   X.shape = (n_samples, n_features)\n",
    "@  w.shape = (n_features, )    ‚áí  y.shape = (n_samples, )\n",
    "```\n",
    "\n",
    "or equivalently\n",
    "\n",
    "```text\n",
    "   X.shape = (n_samples, n_features)\n",
    "@  w_col.shape = (n_features, 1) ‚áí  y.shape = (n_samples, 1)\n",
    "```\n",
    "\n",
    "Because matrix-multiply rules say:\n",
    "\n",
    "- The ‚Äúinner‚Äù dimensions must match (here: `n_features`).\n",
    "- The ‚Äúouter‚Äù dimensions define your output shape (here: `n_samples` √ó `1`).\n",
    "\n",
    "So your mental checklist is:\n",
    "\n",
    "1. **Count your samples** (rows of `X`).  \n",
    "2. **Count your features** (cols of `X`).  \n",
    "3. **Pick or reshape your weight vector** so it has `n_features` entries.  \n",
    "4. **Multiply**‚Äîyou‚Äôll automatically get one result per sample.\n",
    "\n",
    "---\n",
    "\n",
    "### Quick sanity check in code\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "# 5 samples, 3 features\n",
    "X = np.random.randn(5, 3)    # (5, 3)\n",
    "w = np.array([0.2, -0.5, 1.0])  \n",
    "\n",
    "y = X @ w                    # ‚Üí shape (5,)\n",
    "print(y.shape)               # (5,)\n",
    "```\n",
    "\n",
    "If you ever see something like `(3, 5)` or `(1, 5)` instead, you‚Äôve flipped something‚Äîand a quick `.T` will fix it. Just remember: **outer dims give your output shape**!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "207b834c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Neural Network as Curve-Fitter: A Matrix Summary\n",
    "\n",
    "Below is a step-by-step breakdown of the tiny 1-input ‚Üí 2-hidden ‚Üí 1-output network you saw, showing how it really is ‚Äúadding hills‚Äù to fit a curve. Jot this down in your notes!\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Data & Shapes\n",
    "\n",
    "- **Inputs**  \n",
    "  - \\(X\\in\\mathbb{R}^{n\\times 1}\\):  \n",
    "    an \\(n\\)-by-1 column of dosages (one row per sample).\n",
    "\n",
    "- **Hidden layer**  \n",
    "  - Weights \\(W\\in\\mathbb{R}^{1\\times 2}\\) (or equivalently \\(\\mathbb{R}^{2\\times1}\\) depending on convention):  \n",
    "    two hidden neurons, one input each.  \n",
    "  - Biases \\(b\\in\\mathbb{R}^{\\,2}\\):  \n",
    "    one bias per hidden neuron.  \n",
    "  - Pre-activations  \n",
    "    \\[\n",
    "      Z = X\\,W \\;+\\; b\n",
    "      \\quad\\Bigl(Z\\in\\mathbb{R}^{n\\times2}\\Bigr)\n",
    "    \\]\n",
    "  - Activations (‚Äúhills‚Äù)  \n",
    "    \\[\n",
    "      H = \\sigma(Z)\n",
    "      \\quad\\Bigl(H\\in\\mathbb{R}^{n\\times2}\\Bigr)\n",
    "    \\]\n",
    "    where \\(\\sigma\\) = ReLU, sigmoid, etc., applied element-wise.\n",
    "\n",
    "- **Output layer**  \n",
    "  - Weights \\(v\\in\\mathbb{R}^{2\\times1}\\):  \n",
    "    one weight for each hidden neuron feeding into the single output.  \n",
    "  - Bias \\(c\\in\\mathbb{R}\\):  \n",
    "    one bias for the lone output neuron.  \n",
    "  - Final prediction  \n",
    "    \\[\n",
    "      Y = H\\,v \\;+\\; c\n",
    "      \\quad\\Bigl(Y\\in\\mathbb{R}^{n\\times1}\\Bigr)\n",
    "    \\]\n",
    "    giving one efficacy score per sample.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. What‚Äôs Really Happening?\n",
    "\n",
    "1. **Hidden layer**  \n",
    "   - **Multiply** each input \\(x\\) by two slopes (\\(w_1,w_2\\)),  \n",
    "   - **Shift** by biases \\((b_1,b_2)\\),  \n",
    "   - **Squash** through \\(\\sigma\\) ‚Üí two ‚Äúhumps‚Äù \\(h_1(x),h_2(x)\\).\n",
    "\n",
    "2. **Output layer**  \n",
    "   - **Scale** those humps by \\((v_1,v_2)\\),  \n",
    "   - **Sum** them,  \n",
    "   - **Add** the single bias \\(c\\).  \n",
    "   - Result: a composite curve that approximates your target!\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Equations at a Glance\n",
    "\n",
    "\\[\n",
    "\\boxed{\n",
    "\\begin{aligned}\n",
    "\\text{Hidden pre-act:}~~& Z = X\\,W + b, \\\\\n",
    "\\text{Hidden act:}~~~~~~& H = \\sigma(Z), \\\\\n",
    "\\text{Output:}~~~~~~~~~~& Y = H\\,v + c.\n",
    "\\end{aligned}\n",
    "}\n",
    "\\]\n",
    "\n",
    "- **Dimensions** must satisfy:\n",
    "  - \\(X: (n,1)\\),‚ÄÉ\\(W: (1,2)\\)‚ÄÉ‚Üí‚ÄÉ\\(Z: (n,2)\\)  \n",
    "  - \\(H: (n,2)\\),‚ÄÉ\\(v: (2,1)\\)‚ÄÉ‚Üí‚ÄÉ\\(H\\,v: (n,1)\\)  \n",
    "  - + bias \\(c\\) gives final \\((n,1)\\) predictions.\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Role of Biases\n",
    "\n",
    "- **Hidden biases \\(b\\)**: one per hidden neuron ‚Üí vector \\(\\in\\mathbb{R}^2\\).  \n",
    "- **Output bias \\(c\\)**: one for the single output neuron ‚Üí scalar.  \n",
    "- **Rule**: ‚Äú# neurons in the layer‚Äù = ‚Äú# bias terms in that layer.‚Äù\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Why This Matters\n",
    "\n",
    "- **Shape-checking**: Always confirm  \n",
    "  ```python\n",
    "  print(X.shape, W.shape, b.shape, v.shape)\n",
    "  ```\n",
    "  so your matrix multiplies line up and you get an \\((n,1)\\) output.\n",
    "- **Universal Approximation**: More hidden neurons = more little hills ‚Üí more flexible curves.\n",
    "- **Readability**: Stick with ‚Äúrows = samples, columns = features/neurons‚Äù and transpose only to fix mismatches.\n",
    "\n",
    "---\n",
    "\n",
    "üéâ  **TL;DR**: A neural net = **sum of shifted+scaled activation ‚Äúhumps‚Äù** ‚Üí matrix form is  \n",
    "\\[\n",
    "Y = \\sigma(XW + b)\\,v + c.\n",
    "\\]  \n",
    "Keep track of your shapes and biases, and you‚Äôve got the full curve-fitting picture in one neat formula."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70e99b87",
   "metadata": {},
   "source": [
    "Sure‚Äîlet‚Äôs write it out in the classic ‚Äúlayer-by-layer‚Äù notation you‚Äôll see in most textbooks.\n",
    "\n",
    "---\n",
    "\n",
    "## 1-Hidden-Layer Network (1 ‚Üí 2 ‚Üí 1)\n",
    "\n",
    "Let  \n",
    "- \\(x\\in\\mathbb{R}\\) be your scalar input (dosage).  \n",
    "- \\(w_i, b_i\\) be the weight and bias of hidden unit \\(i\\in\\{1,2\\}\\).  \n",
    "- \\(v_i, c\\) be the weight and bias of the single output neuron.\n",
    "\n",
    "**Hidden layer pre-activation**  \n",
    "\\[\n",
    "z_i^{[1]} \\;=\\; w_i\\,x \\;+\\; b_i\n",
    "\\quad(i=1,2)\n",
    "\\]\n",
    "\n",
    "**Hidden activation**  \n",
    "\\[\n",
    "a_i^{[1]} \\;=\\; \\sigma\\bigl(z_i^{[1]}\\bigr)\n",
    "\\quad\\Longrightarrow\\quad\n",
    "a^{[1]} = \n",
    "\\begin{bmatrix}a_1^{[1]} \\\\ a_2^{[1]}\\end{bmatrix}\n",
    "\\]\n",
    "\n",
    "**Output pre-activation**  \n",
    "\\[\n",
    "z^{[2]} \\;=\\; \\sum_{i=1}^{2} v_i\\,a_i^{[1]} \\;+\\; c\n",
    "\\]\n",
    "\n",
    "**Output activation (identity for regression)**  \n",
    "\\[\n",
    "\\hat y\n",
    "=\\; a^{[2]} \\;=\\; z^{[2]}\n",
    "\\]\n",
    "\n",
    "Putting it all together:\n",
    "\\[\n",
    "\\boxed{ \n",
    "\\hat y\n",
    "= c \\;+\\; \\sum_{i=1}^{2} v_i\\,\\sigma\\bigl(w_i\\,x + b_i\\bigr)\n",
    "}\n",
    "\\]\n",
    "\n",
    "---\n",
    "\n",
    "## Matrix Form\n",
    "\n",
    "Define\n",
    "\\[\n",
    "W^{[1]} = \\begin{bmatrix}w_1\\\\ w_2\\end{bmatrix},\\quad\n",
    "b^{[1]} = \\begin{bmatrix}b_1\\\\ b_2\\end{bmatrix},\\quad\n",
    "W^{[2]} = \\begin{bmatrix}v_1 & v_2\\end{bmatrix},\\quad\n",
    "b^{[2]} = \\bigl[c\\bigr].\n",
    "\\]\n",
    "Then for an input vector \\(x\\) (or a batch \\(X\\in\\mathbb R^{n\\times1}\\)),\n",
    "\n",
    "1. **Layer 1**  \n",
    "   \\[\n",
    "   z^{[1]} = W^{[1]}\\,x \\;+\\; b^{[1]}, \n",
    "   \\quad\n",
    "   a^{[1]} = \\sigma\\!\\bigl(z^{[1]}\\bigr).\n",
    "   \\]\n",
    "2. **Layer 2**  \n",
    "   \\[\n",
    "   z^{[2]} = \\bigl(W^{[2]}\\,a^{[1]}\\bigr) \\;+\\; b^{[2]},\n",
    "   \\quad\n",
    "   a^{[2]} = z^{[2]}.\n",
    "   \\]\n",
    "\n",
    "So compactly:\n",
    "\\[\n",
    "\\boxed{\n",
    "a^{[2]} \n",
    "= W^{[2]}\\,\\sigma\\!\\bigl(W^{[1]}x + b^{[1]}\\bigr)\n",
    "\\;+\\; b^{[2]}.\n",
    "}\n",
    "\\]\n",
    "\n",
    "---\n",
    "\n",
    "### General L-Layer Network\n",
    "\n",
    "For layer \\(\\ell=1,\\dots,L\\), denote weights \\(W^{[\\ell]}\\), biases \\(b^{[\\ell]}\\), activations \\(a^{[\\ell]}\\). Then\n",
    "\n",
    "\\[\n",
    "\\begin{aligned}\n",
    "z^{[\\ell]} &= W^{[\\ell]}\\,a^{[\\ell-1]} + b^{[\\ell]},\\\\\n",
    "a^{[\\ell]} &= \n",
    "\\begin{cases}\n",
    "\\sigma\\bigl(z^{[\\ell]}\\bigr), & \\ell < L,\\\\[4pt]\n",
    "\\text{identity}(z^{[\\ell]}), & \\ell = L.\n",
    "\\end{cases}\n",
    "\\end{aligned}\n",
    "\\]\n",
    "with \\(a^{[0]} = x\\).  \n",
    "\n",
    "That‚Äôs the conventional equation set you‚Äôll find in any deep-learning text‚Äîevery network is just these two lines per layer!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3072c28",
   "metadata": {},
   "source": [
    "Thought for a couple of seconds\n",
    "\n",
    "\n",
    "Here‚Äôs the TL;DR in your own style:\n",
    "\n",
    "* **Single sample** (3 inputs ‚Üí 7 hidden ‚Üí 2 output):\n",
    "\n",
    "  ```\n",
    "  (1√ó3) √ó (3√ó7) ‚Üí (1√ó7)  ‚Üí (1√ó7) √ó (7√ó2) ‚Üí (1√ó2)\n",
    "  ```\n",
    "\n",
    "  * Multiply your 1√ó3 input by a 3√ó7 weight ‚áí 1√ó7\n",
    "  * Add a 1√ó7 bias (broadcasting happens automatically) ‚áí still 1√ó7\n",
    "  * Activate (ReLU, tanh, whatever) ‚áí 1√ó7\n",
    "  * Multiply by a 7√ó2 weight ‚áí 1√ó2\n",
    "  * Add a 1√ó2 bias ‚áí final 1√ó2 output\n",
    "\n",
    "* **Batch of 5 samples** (3 inputs ‚Üí 4 hidden ‚Üí 2 output):\n",
    "\n",
    "  ```\n",
    "  X: (5√ó3)\n",
    "  W‚ÇÅ: (3√ó4),  b‚ÇÅ: (1√ó4)    ‚áí  Z‚ÇÅ = X @ W‚ÇÅ + b‚ÇÅ  ‚Üí (5√ó4)\n",
    "  W‚ÇÇ: (4√ó2),  b‚ÇÇ: (1√ó2)    ‚áí  Z‚ÇÇ = A‚ÇÅ @ W‚ÇÇ + b‚ÇÇ  ‚Üí (5√ó2)\n",
    "  ```\n",
    "\n",
    "* **Bias shape** is always ‚Äú1√ónumber\\_of\\_nodes‚Äù (or a length-n\\_nodes 1D array) so it neatly broadcasts across all your samples.\n",
    "\n",
    "* **Alternative convention** (samples as columns):\n",
    "\n",
    "  ```\n",
    "  X: (3√ó5), W: (7√ó3), b: (7√ó1) ‚áí Z = W @ X + b ‚Üí (7√ó5)\n",
    "  ```\n",
    "\n",
    "  but 99% of Python code uses rows-as-samples.\n",
    "\n",
    "So yep‚Äîbias is always 1√ó\\[#nodes], and your chain of matrix dims is exactly what you‚Äôve been thinking. üöÄ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "33b07586",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ba90ce9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = np.array([\n",
    "  [1, 2, 3],\n",
    "  [6, 7, 8],\n",
    "  [7, 8, 9],\n",
    "  [3, 4, 5],\n",
    "  [4, 5, 6],\n",
    "])\n",
    "\n",
    "# By convention, inputs in matrix form are often denoted by a capital X.\n",
    "X = samples\n",
    "\n",
    "targets = np.array([False, True, True, False, False])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7770dc9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "uniques, indicies = np.unique(targets, return_inverse=True)\n",
    "# Return inverse means that from the indecies returned in that array if you then use them to call the unique values, you can construct the original data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b997f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 1, 0, 0], dtype=int64)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "binary = uniques.shape[0]\n",
    "n_samples = samples.shape[0]\n",
    "y = np.zeros((n_samples,binary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "85c90aae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[np.arange(n_samples),indicies] = 1\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "9902ee8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.72433282 -2.26463532  0.08585342  2.13004535]\n",
      " [-1.02964036 -8.09519327  0.17120801  6.94368505]\n",
      " [-1.09070187 -9.26130486  0.18827893  7.90641299]\n",
      " [-0.84645584 -4.5968585   0.11999526  4.05550123]\n",
      " [-0.90751735 -5.76297009  0.13706617  5.01822917]]\n",
      "[[ 0.27566718 -1.26463532  1.08585342  3.13004535]\n",
      " [-0.02964036 -7.09519327  1.17120801  7.94368505]\n",
      " [-0.09070187 -8.26130486  1.18827893  8.90641299]\n",
      " [ 0.15354416 -3.5968585   1.11999526  5.05550123]\n",
      " [ 0.09248265 -4.76297009  1.13706617  6.01822917]]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(10)\n",
    "\n",
    "n_features = X.shape[1]\n",
    "n_hidden = 4\n",
    "w_h = np.random.uniform(-0.5,0.5,(n_features,n_hidden))\n",
    "b_h = np.zeros((1,n_hidden))\n",
    "b_h_show = np.ones((1,n_hidden))\n",
    "print(np.dot(X,w_h))\n",
    "print(X@w_h + b_h_show)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "7dfba257",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.08585342, 2.13004535],\n",
       "       [0.        , 0.        , 0.17120801, 6.94368505],\n",
       "       [0.        , 0.        , 0.18827893, 7.90641299],\n",
       "       [0.        , 0.        , 0.11999526, 4.05550123],\n",
       "       [0.        , 0.        , 0.13706617, 5.01822917]])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h1 = X @ w_h + b_h\n",
    "a1 = np.maximum(0,h1) # ReLu...\n",
    "a1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "fba4c24d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.32118171, 0.66159156],\n",
       "       [1.10083177, 2.19782851],\n",
       "       [1.25676178, 2.5050759 ],\n",
       "       [0.63304174, 1.27608634],\n",
       "       [0.78897175, 1.58333373]])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(100)\n",
    "\n",
    "w_o  = np.random.uniform(-0.5,0.5,(n_hidden,binary))\n",
    "b_o = np.zeros((1,binary))\n",
    "o = a1 @ w_o + b_o\n",
    "o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "821001db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.41570992, 0.58429008],\n",
       "       [0.25030304, 0.74969696],\n",
       "       [0.22299211, 0.77700789],\n",
       "       [0.34455863, 0.65544137],\n",
       "       [0.31123284, 0.68876716]])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e_x = np.exp(o - o.max(axis=1, keepdims=True))\n",
    "\n",
    "y_hat = e_x/np.sum(e_x,1,keepdims=True)\n",
    "y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e155ece",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.58429008, -0.58429008],\n",
       "       [-0.25030304,  0.25030304],\n",
       "       [-0.22299211,  0.22299211],\n",
       "       [ 0.65544137, -0.65544137],\n",
       "       [ 0.68876716, -0.68876716]])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18bc22b0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
