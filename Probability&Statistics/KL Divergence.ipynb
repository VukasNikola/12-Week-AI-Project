{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "56f6c428",
   "metadata": {},
   "source": [
    "Kullback Leibler divergence! The idea is that we want to calculate the distance between probability distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "064e2703",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "coin1 = np.array([0.5,0.5])\n",
    "p = 0.4\n",
    "q = 1 - p\n",
    "coin2 = np.array([p,q])\n",
    "# Depending on the p and q we can say how close the coin2 is to coin1, if p = 0.55 we can say it is close. Not really if p = 0.95\n",
    "# The idea is that you can notice a difference with the p=0.95 coin with the fair one through just a few tosses, since it will result in lots of Heads, unlike the fair one.\n",
    "# You basically see if the distributions assigned similar probabilities to a similar sequences\n",
    "# If they assign similar probabilities then we can say that the distributions are similar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "43e92291",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0204, dtype=torch.float64)\n",
      "0.020410997260127586\n"
     ]
    }
   ],
   "source": [
    "kl = coin1[0]*np.log(coin1[0]/coin2[0]) + coin1[1]*np.log(coin1[1]/coin2[1])\n",
    "c2 = torch.tensor(coin2)\n",
    "c1 = torch.tensor(coin1)\n",
    "\n",
    "print(torch.kl_div(torch.log(c2),c1,reduction = 2))\n",
    "print(kl)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef1de8d5",
   "metadata": {},
   "source": [
    "Here‚Äôs the **intuition and derivation** of KL divergence straight from the video, illustrated with our fair-vs-biased-coin example:\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Imagine two coins\n",
    "\n",
    "* **True coin** $P$: fair, $P(\\mathrm{H})=0.5,\\;P(\\mathrm{T})=0.5$\n",
    "* **Assumed coin** $Q$: biased toward heads, $Q(\\mathrm{H})=0.9,\\;Q(\\mathrm{T})=0.1$\n",
    "\n",
    "You flip one of them $N$ times and record your sequence of H/T.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Compare likelihoods of a sequence\n",
    "\n",
    "For a particular sequence with $k$ heads and $N-k$ tails:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "P(\\text{seq}) &= 0.5^k\\;0.5^{\\,N-k} \\;,\\\\\n",
    "Q(\\text{seq}) &= 0.9^k\\;0.1^{\\,N-k}\\;.\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "If $Q$ really matched $P$, those two numbers would be about equal.\n",
    "\n",
    "Take the **ratio**:\n",
    "\n",
    "$$\n",
    "\\frac{P(\\text{seq})}{Q(\\text{seq})}\n",
    "= \\Bigl(\\tfrac{0.5}{0.9}\\Bigr)^k\n",
    "  \\;\\Bigl(\\tfrac{0.5}{0.1}\\Bigr)^{N-k}.\n",
    "$$\n",
    "\n",
    "Then take a log (to turn products into sums and tame tiny numbers) and **normalize** by $N$ to get an average ‚Äì that‚Äôs exactly:\n",
    "\n",
    "$$\n",
    "D_{KL}(P\\|Q)\n",
    "= \\sum_{x\\in\\{H,T\\}} P(x)\\,\\ln\\frac{P(x)}{Q(x)}.\n",
    "$$\n",
    "\n",
    "This derivation is laid out step by step in the video üëá ([YouTube][1], [Í≥µÎ∂ÄÎ∞©][2])\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Plug in our numbers\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "D_{KL}(P\\|Q)\n",
    "&= 0.5\\ln\\frac{0.5}{0.9}\n",
    "+ 0.5\\ln\\frac{0.5}{0.1}\\\\[4pt]\n",
    "&\\approx 0.5(-0.5878)+0.5(1.6094)\n",
    "= 0.5108\\text{ nats}\n",
    "\\quad\\bigl(\\approx0.737\\text{ bits}\\bigr).\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "So on average you‚Äôd get about **0.51 ‚Äúnats‚Äù of surprise** each flip by believing the heavily-biased coin when the true coin is fair. üò≤\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Key take-aways\n",
    "\n",
    "* **Asymmetry**: $D_{KL}(P\\|Q)\\neq D_{KL}(Q\\|P)$. Swapping them gives a different ‚Äúsurprise.‚Äù\n",
    "* **Zero only if** $P=Q$. A model that matches reality incurs no extra surprise.\n",
    "* **Uses**: model comparison, coding theory (extra bits wasted), variational inference (as a loss).\n",
    "\n",
    "That‚Äôs the heart of the video‚Äôs coin-flip story ‚Äì a natural, average log-ratio of ‚Äúhow much more surprising‚Äù your data is under the wrong coin. üé≤‚ú®\n",
    "\n",
    "[1]: https://www.youtube.com/watch?pp=0gcJCdgAo7VqN5tD&v=SxGYPqCgJWM&utm_source=chatgpt.com \"Intuitively Understanding the KL Divergence - YouTube\"\n",
    "[2]: https://mookstudy.tistory.com/12?utm_source=chatgpt.com \"KL Divergence Ïú†ÎèÑ Í≥ºÏ†ï ÏßÑÏßú ÏâΩÍ≤å ÏïåÏïÑÎ≥¥Í∏∞ - Í≥µÎ∂ÄÎ∞© - Ìã∞Ïä§ÌÜ†Î¶¨\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3c9f3bd",
   "metadata": {},
   "source": [
    "KL divergence is the average ‚Äúextra surprise‚Äù you pay per flip (in nats or bits) by assuming a biased coin when the true coin is fair. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f21b5466",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
